#### 聚类算法
    注意力

#### 相似度
    不同的距离测度，结果不一样
        1. 通过L2范数判断相似度
        2. 

#### 距离
    P=1： 曼哈顿距离
        d(x,y) = |x1 - y1| + |x2 - y2| + ... + |xn - yn|
    P=2： 欧氏距离
        d(x,y) = √(x1-y1)^2
    P=无穷: 
        切比雪夫距离 那个维度差值最大就是哪个差值作为距离
    闵可夫斯基距离:
        d(x,y) = p√(|x1 - y1|^p + ... + |x2 - y2|^p)
    
    余弦距离: 将数据看做空间中的点的时候，评价远近可以用欧氏距离或者余弦距离
        将数据映射为高维空间中的点（向量）
        计算向量间的余弦值
        取值范围[-1,+1] 越趋近于1代表越相似，越趋近于-1代表方向相反，0 代表正交
        
        cosθ = (a • b) / (||a|| ||b||)
        cosθ = (x1x2 + y1y2) / √((x1^2 + y1^2) * (x2^2 + y2^2))


#### TF-IDF
    TF: 词频, 在给定的文档中某个词出现的概率, 某篇文章内部 某词出现的次数/文章的总词数
    DF: 语料库中包含词t的总文章数
    IDF: 逆向文本频率, 
        idf = lg(|D| / (df + 1))
    tf-idf = tf * idf
    
#### 数据相似度 - Jaccard相似系数
    用来衡量有限样本集之间的相似程度
    J(A,B) = |A ∩ B| / |A ∪ B| = |A n B| / (|A| + |B| - |A n B|)
    
    可以应用于 网页去重、文本相似度分析

    PRECISION: 给出的正确中有多少正确的
    Recall: 所有的正确中有多少给出了
    
#### K-means
    算法步骤
        选择K个初始的簇中心
            怎么选？
        逐个计算每个样本到中心的距离，将样本归属到距离最小的那个簇中心的簇中
        每个簇内部计算平均值，更新簇中心
        开始迭代

![图片alt](./images/K-means算法流程.png)
    
    K-means可以work的理论基础是假定了数据点符合同方差的高斯分布模型
    通过最大似然估计得到的k-means的迭代方法
    这个函数是个非凸函数，根据初始值不同只能得到局部最优解
    KMeans损失函数
    
    

    优点:
        简单，效果不错
    缺点:
        对异常值敏感: 数据预处理的时候过滤离群值
        对初始值敏感: 
        对某些分布聚类效果不好

    K-Mediods 
        计算新的簇中心的时候不再选择均值，而是选择中位数
        抗噪能力得到加强
    
    二分K-means
        K-means的损失函数
        K-means的损失函数
        分别计算四个簇的mse，会发现有两个簇的MSE很小，一个簇的MSE很大
        选择合并簇中心点比较近，MSE很小簇，切分簇中心离其他簇中心比较远，MSE比较大的簇，重新进行K-means聚类
    
    K-means++
        K-means选择一个好的初始中心点非常重要
        K-means++ 改变初始中心点的位置
        目标：初始化簇中心点稍微远一些
        
        步骤：
            随机选择第一个中心点
            计算每个样本到第一个中心点的距离
            将距离转化为概率
            概率化选择

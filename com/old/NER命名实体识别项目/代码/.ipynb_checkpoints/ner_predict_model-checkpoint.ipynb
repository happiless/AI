{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import os\n",
    "from gensim.models.word2vec import Word2Vec, PathLineSentences\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow as tf\n",
    "\n",
    "class OneHot(object):\n",
    "    def __init__(self):\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.__onehot_encodeder = OneHotEncoder()\n",
    "\n",
    "    def encode(self, target_list):\n",
    "        integer_encoded = self.label_encoder.fit_transform(np.array(target_list))\n",
    "        integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "        self.__onehot_encodeder = self.__onehot_encodeder.fit_transform(integer_encoded)\n",
    "        return self.__onehot_encodeder.toarray()\n",
    "\n",
    "    def encode_label(self, target_list):\n",
    "        integer_encoded = self.label_encoder.fit_transform(np.array(target_list))\n",
    "        return integer_encoded\n",
    "\n",
    "    def decode(self, encoder_list):\n",
    "        return self.label_encoder.inverse_transform(np.argmax(np.array(encoder_list), axis=1))\n",
    "\n",
    "\n",
    "def read_file_to_corpus(folder):\n",
    "    corpus = []\n",
    "    for filename in os.listdir(folder):\n",
    "        with open(os.path.join(folder, filename), encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                corpus.append(line.split())\n",
    "    return corpus\n",
    "\n",
    "\n",
    "\n",
    "def get_vec_model(model_path):\n",
    "    vec_model = gensim.models.Word2Vec.load(model_path)\n",
    "    return vec_model\n",
    "\n",
    "\n",
    "def get_train_list(source_folder, target_folder):\n",
    "    source_string = []\n",
    "    target_string = []\n",
    "    for filename in os.listdir(source_folder):\n",
    "        target_file_name = \"targetH_\" + \"_\".join(filename.split(\"_\")[1:])\n",
    "        if os.path.exists(os.path.join(target_folder, target_file_name)):\n",
    "            with open(os.path.join(source_folder, filename), encoding=\"utf-8\") as source:\n",
    "                with open(os.path.join(target_folder, target_file_name), encoding=\"utf-8\") as target:\n",
    "                    for source_line in source:\n",
    "                        for target_line in target:\n",
    "                            if len(source_line.split()) == len(target_line.split()):\n",
    "                                source_string.append(source_line.split())\n",
    "                                target_string.append(target_line.split())\n",
    "    return source_string, target_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_feature(source_string, vec_model, max_sequence=1000):\n",
    "    index2word_set = set(vec_model.wv.index2word)\n",
    "    row_vector_list = []\n",
    "    for source_line in source_string:\n",
    "        i = 0\n",
    "        row_vector = []\n",
    "        for source_word in source_line:\n",
    "            if i < max_sequence:\n",
    "                if source_word in index2word_set:\n",
    "                    row_vector= np.append(row_vector, vec_model[source_word])\n",
    "                else:\n",
    "                    row_vector = np.append(row_vector, np.zeros(vec_model.trainables.layer1_size, dtype='float32'))\n",
    "            i += 1\n",
    "        if len(source_line) < max_sequence:\n",
    "            row_vector = np.append(row_vector,\n",
    "                                   np.zeros((vec_model.trainables.layer1_size * (max_sequence - len(source_line)),),\n",
    "                                            dtype='float32'))\n",
    "        row_vector_list.append(row_vector)\n",
    "    return np.matrix(row_vector_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_target_label(target_string,max_sequence=1000):\n",
    "    onehot_model = OneHot()\n",
    "    for i in range(0, len(target_string)):\n",
    "        if len(target_string[i]) < max_sequence:\n",
    "            target_string[i] = target_string[i].extend([\"O\"]*(max_sequence - len(target_string[i])))\n",
    "            if target_string[i] is None:\n",
    "                target_string[i] = [\"O\"]*max_sequence\n",
    "        else:\n",
    "            if target_string[i] is None:\n",
    "                target_string[i] = [\"O\"]*max_sequence\n",
    "            else:\n",
    "                target_string[i] = target_string[i][0:max_sequence]\n",
    "    num_rows = len(target_string)\n",
    "    flat_list = [item for sublist in target_string for item in sublist]\n",
    "    print(flat_list)\n",
    "    target_vector = onehot_model.encode_label(flat_list)\n",
    "    target_vector = target_vector.reshape(-1, max_sequence)\n",
    "    return target_vector, onehot_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import os\n",
    "from gensim.models.word2vec import Word2Vec, PathLineSentences\n",
    "import numpy as np\n",
    "def get_train_list(source_folder, target_folder):\n",
    "    source_string = []\n",
    "    target_string = []\n",
    "    for filename in os.listdir(source_folder):\n",
    "        target_file_name = \"targetH_\" + \"_\".join(filename.split(\"_\")[1:])\n",
    "        if os.path.exists(os.path.join(target_folder, target_file_name)):\n",
    "            with open(os.path.join(source_folder, filename), 'r', encoding=\"utf-8\") as source:\n",
    "                with open(os.path.join(target_folder, target_file_name), 'r', encoding=\"utf-8\") as target:\n",
    "                    for source_line, target_line in zip(source.readlines(), target.readlines()):\n",
    "                        s_line = source_line.split()\n",
    "                        t_line = target_line.split()\n",
    "                        if len(s_line) == len(t_line):\n",
    "                            source_string.append(s_line)\n",
    "                            target_string.append(t_line)\n",
    "    print('源数据读取完毕，共' + str(len(source_string)) + '行')\n",
    "    return source_string, target_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vec_from_corpus(corpus, size=128, min_count=2, save_path=os.path.join(\"./data/ner_word2vec_model\")):\n",
    "    vec_model = Word2Vec(sentences=corpus, size=size, min_count=min_count)\n",
    "    vec_model.save(save_path)\n",
    "    return vec_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_crf(X):\n",
    "    embedding_size = 128\n",
    "    unit_num = 128\n",
    "    dropout_rate = None\n",
    "    output_size = 3\n",
    "    batch_size = 1\n",
    "    seq_length = 10\n",
    "    lr = 0.001\n",
    "\n",
    "    cell_forward = tf.nn.rnn_cell.BasicLSTMCell(unit_num)\n",
    "    cell_backward = tf.nn.rnn_cell.BasicLSTMCell(unit_num)\n",
    "    input_bi_lstm = tf.reshape(X, [batch_size, seq_length, embedding_size])\n",
    "    bi_outputs, bi_state = tf.nn.bidirectional_dynamic_rnn(cell_forward,\n",
    "                                    cell_backward, input_bi_lstm, dtype=tf.float32)\n",
    "\n",
    "    bi_output = tf.concat(bi_outputs, axis=2)\n",
    "\n",
    "    W = tf.get_variable(\"projection_w\", [2 * unit_num, output_size])\n",
    "    b = tf.get_variable(\"projection_b\", [output_size])\n",
    "    x_reshape = tf.reshape(bi_output, [-1, 2 * unit_num])\n",
    "    projection = tf.matmul(x_reshape, W) + b\n",
    "    outputs = tf.reshape(projection, [batch_size, seq_length, output_size])\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(predcit_feature, model_path):\n",
    "    embedding_size = 128\n",
    "    unit_num = 128\n",
    "    dropout_rate = None\n",
    "    output_size = 3\n",
    "    batch_size = 1\n",
    "    seq_length = 10\n",
    "    lr = 0.001\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, shape=[batch_size, seq_length * embedding_size])\n",
    "    pred = lstm_crf(X)\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    predict_label=[]\n",
    "    with tf.Session() as sess:\n",
    "        #参数恢复\n",
    "        module_file = tf.train.latest_checkpoint(model_path)\n",
    "        saver.restore(sess, module_file)\n",
    "        for step in range(len(predcit_feature)):\n",
    "            print(step)\n",
    "            prob = sess.run(pred, feed_dict={X:predcit_feature[step]})\n",
    "            predict=prob.reshape((-1)).reshape(-1,3)\n",
    "            predict_label.append(predict)\n",
    "    return predict_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "源数据读取完毕，共2行\n",
      "['O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "source_string, target_string = get_train_list(\"./data/source\",\"./data/target\")\n",
    "vec_model = get_vec_from_corpus(source_string, min_count=1)\n",
    "target_vector, onehot_model = get_target_label(target_string, max_sequence=10)\n",
    "feature = get_train_feature(source_string, vec_model, max_sequence=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, (2, 1280))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature), np.array(feature).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/bilstm-400\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "predict_result = predict(feature, \"./model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-e448727e24c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "len(predict_result), np.array(predict_result).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[  2.7013447,   8.644368 ,  -8.853404 ],\n",
       "        [  6.632493 ,   1.5225788,  -6.3457103],\n",
       "        [  1.134716 ,   3.4497828,  -6.0142875],\n",
       "        [-12.048701 ,  12.990964 ,  -9.226457 ],\n",
       "        [-16.25568  ,  15.564315 ,  -9.794048 ],\n",
       "        [-17.570583 ,  16.039436 ,  -9.477436 ],\n",
       "        [-18.120975 ,  16.190962 ,  -9.253028 ],\n",
       "        [-18.397783 ,  16.258446 ,  -9.117073 ],\n",
       "        [-18.551426 ,  16.286884 ,  -9.025663 ],\n",
       "        [-18.628977 ,  16.285948 ,  -8.963935 ]], dtype=float32),\n",
       " array([[  2.7013447,   8.644368 ,  -8.853404 ],\n",
       "        [  6.632493 ,   1.5225788,  -6.3457103],\n",
       "        [  1.134716 ,   3.4497828,  -6.0142875],\n",
       "        [-12.048701 ,  12.990964 ,  -9.226457 ],\n",
       "        [-16.25568  ,  15.564315 ,  -9.794048 ],\n",
       "        [-17.570583 ,  16.039436 ,  -9.477436 ],\n",
       "        [-18.120975 ,  16.190962 ,  -9.253028 ],\n",
       "        [-18.397783 ,  16.258446 ,  -9.117073 ],\n",
       "        [-18.551426 ,  16.286884 ,  -9.025663 ],\n",
       "        [-18.628977 ,  16.285948 ,  -8.963935 ]], dtype=float32)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 3)\n",
      "[1 0 1 1 1 1 1 1 1 1]\n",
      "['O' 'B-ORG' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O']\n",
      "(10, 3)\n",
      "[1 0 1 1 1 1 1 1 1 1]\n",
      "['O' 'B-ORG' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O']\n"
     ]
    }
   ],
   "source": [
    "for line in predict_result:\n",
    "    print(line.shape)\n",
    "    print(np.argmax(np.array(line), axis=1))\n",
    "    predict_label = onehot_model.decode(line)\n",
    "    print(predict_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
